
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>REC-MV</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel='icon' href='/icons/icon-bulb-2-512.png' sizes='512x512'>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>


    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                <b>REC-MV: REconstructing 3D Dynamic Cloth<br> from Monucular Videos <br>
                <small>
                    CVPR 2023
                </small>
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://lingtengqiu.github.io/">
                            Lingteng Qiu
                        </a><sup>1,2,3*</sup>
                    </li>
                    <li>
                        <a href="https://guanyingc.github.io/">
                            Guanying Chen
                        </a><sup>1,2,3*</sup>
                    </li>
                    <li>
                            Jiapeng Zhou<sup>2,3</sup>
                    </li><br>
                    <li>
                        <a href="https://mutianxu.github.io/">
                            Mutian Xu
                        </a><sup>1,3</sup>
                    </li>
                    <li>
                            Junle Wang<sup>4</sup>
                    </li>
                    <li>
                        <a href="https://mypage.cuhk.edu.cn/academics/hanxiaoguang/">
                            Xiaoguang Han
                        </a><sup>1,2,3#</sup>
                    </li>
                    <!-- Inc. -->
                    <br>
                    <li>
                        <small>
                            <sup>1</sup>CUHK-SZ
                        </small>
                    </li>
                    <li>
                        <small>
                            <sup>2</sup>FNii
                        </small>
                        
                    </li>
                    <li>
                        <small><sup>3</sup>SSE</small>
                        
                    </li>
                    <li>
                        <small><sup>4</sup>Tencent</small>
                    </li>
                </ul>
            </div>
        </div>



        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                            <image src="./icons/paper.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="./icons/youtube.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/lingtengqiu/REC-MV">
                            <image src="./icons/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="./images/teaser.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Reconstructing dynamic 3D garment surfaces with open
                    boundaries from monocular videos is an important problem
                    as it provides a practical and low-cost solution for clothes
                    digitization. Recent neural rendering methods achieve
                    high-quality dynamic clothed human reconstruction results
                    from monocular video, but these methods cannot separate
                    the garment surface from the body. Moreover, despite existing garment reconstruction methods based on feature
                    curve representation demonstrating impressive results for
                    garment reconstruction from a single image, they struggle
                    to generate temporally consistent surfaces for the video input. To address the above limitations, in this paper, we formulate this task as an optimization problem of 3D garment
                    feature curves and surface reconstruction from monocular
                    video. We introduce a novel approach, called REC-MV, to
                    jointly optimize the explicit feature curves and the implicit
                    signed distance field (SDF) of the garments. Then the open
                    garment meshes can be extracted via garment template registration in the canonical space. Experiments on multiple
                    casually captured datasets show that our approach outperforms existing methods and can produce high-quality dynamic garment surfaces.                               
                </p><br>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="./media/demo.mp4" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Methodology
                </h3>
                <p class="text-justify">
                    Given a monocular video with Ni frames depicting a moving person {It|t = 1, . . . , Ni}, REC-MV aims to reconstruct high-fidelity and space-time coherent open garment meshes. This is a challenging problem as it requires a
                    method to simultaneously capture the shape contours, local surface details, and the motion of the garment. Observing that feature curves (e.g., necklines, hemlines) provide critical cues for 
                    determining the shape contours of garment and implicit signed distance function (SDF) can well represent a detailed closed surface, we propose to first optimize the explicit 3D feature curves and
                    implicit garment surfaces from the video, and then apply non-rigid clothing template registration to extract the open garment meshes.
                </p><br>
                <image src="./images/pipeline.png" class="img-responsive" alt="method"><br>
                <p class="text-justify">
                    Our method can be devided into 4 parts:<br>
                    (a) Starting from a surface template, we initialize the canonical curves by solving Eq. (3), and apply a handle-based deformation to initialize the canonical implicit surface.<br>
                    (b) Given an i-th frame, canonical curves are deformed to the camera view space to compute the projection loss based on the surface-aware visibility estimation. <br>
                    (c) Similarly, the canonical surface  is deform to the camera view to compute the photometric loss by differentiable rendering. The curves and surface are jointly optimized to enable a progressive co-evolution.<br>
                    (d) Last, the open garment meshes can be extracted by template registration in the canonical space.
                </p><br>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Qualitative Results
                </h3>
                <image src="./images/qualitative_results(a).png" class="img-responsive" alt="overview"><br>
                <image src="./images/qualitative_results(b).png" class="img-responsive" alt="overview"><br>
                <image src="./images/qualitative_results(e).png" class="img-responsive" alt="overview"><br>
                <p class="text-center">
                    <strong>
                        Qualitative comparison on real datasets between BCNet, ClothWild, ReEF, and our method
                    </strong>
                </p>
                <image src="./images/qualitative_results(d).png" class="img-responsive" alt="overview"><br>
                <p class="text-center">
                    <strong>
                        Reconstruction results on a large pose sequence
                    </strong>
                </p><br>

                <image src="./images/qualitative_results(c).png" class="img-responsive" alt="overview"><br>
                <p class="text-center">
                    <strong>
                        Dynamic garment reconstruction results of our method
                    </strong>
                </p>

            </div>
        </div>

            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
                    @inproceedings{qiu2023REC-MV,

                    year={2023}
                    }
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    <!-- This work was partially supported by the National Key R&D Program of China (No.2018YFB1800800), the Basic Research Project No.~HZQB-KCZYZ-2021067 of Hetao Shenzhen-HK S&T Cooperation Zone, and Hong Kong RGC GRF grant (project# 17203119).
                    <br> -->
                    The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>