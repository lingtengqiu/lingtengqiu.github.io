<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="author" content="Lingteng Qiu">
  <meta name="description" content="Lingteng Qiu's Homepage"
  <meta name="keywords" content="Lingteng Qiu,邱陵腾,homepage,主页, PhD, computer vision, CUHK-SZ, 3D reconstruction, Pose estimation, Neural rendering, Digital Garment>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Lingteng Qiu (邱陵腾)'s Homepage</title>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Lingteng Qiu &nbsp; &nbsp;邱陵腾</name>
              </p>
              <p style="text-align:center">
                <a href="mailto:220019047@link.cuhk.edu.cn">Email</a>
                &nbsp; &nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=YJFpZ2kAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a> &nbsp; &nbsp;&nbsp;&nbsp;<a href="https://github.com/lingtengqiu">Github</a>
              </p>

              <div class="w3-content" style="text-align: justify">
              <p>I am a Ph.D student (from Sep. 2020) of <a href="https://sse.cuhk.edu.cn/">School of Science and Engineering, The Chinese University of Hongkong, Shenzhen</a> and under the supervision of <a href="https://mypage.cuhk.edu.cn/academics/hanxiaoguang/">Prof. Xiaoguang Han</a>. 
                Prior to CUHK-Shenzhen, I obtained M.S. from <a href="http://www.hit.edu.cn/">Department of Automation, Harbin Institue of Technology</a> and B.S. from <a href="https://www.fzu.edu.cn/">Fuzhou University</a> (Rank: 1/113). 
                <br>
                <br>
                My research interests are in computer vision and machine learning. Specifically, learning based methods 
                for 3D vision, human pose estimation, neural rendering, AIGC, etc.
              </p>
              </div>
            </td>
            <td style="padding:10% 3% 3% 3%;width:40%;max-width:40%">
              <a href="images/Lingtengqiu.jpg"><img style="width:70%;max-width:120%" alt="profile photo" src="images/Lingtengqiu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


				<table width="92%" align="left" border="0" cellspacing="0" style="padding-left:20px;padding-bottom:5px"><tbody>
				<tr> <td style=padding:00px;width:90%;vertical-align:middle;padding-left:00px>
						<font color="red"><strong><highlight>&#9733; 
              I will be on job market in 2024, and am actively looking for research internship opportunity in Spring 2024, and full-time job in big companies or start-ups after Summer 2024.
							</highlight></strong></font>
						</br> </br> </br>
						</td> </tr>
				</tbody> </table> 



        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              2023-02: 	One paper is accepted by CVPR 2023, more details comming soon!<br>
              2022-03: 	Two papers are accepted by CVPR 2022!<br>
              2021-03: 	One paper is accepted by CVPR 2021!<br>
              2020-07: 	One paper is accepted by ECCV 2020! Code and dataset are <a href="https://github.com/lingtengqiu/OPEC-Net">available</a>.
            </p>
          </td>
        </tr>
      </tbody></table>



      <table width="100%" align="center" border="0" cellspacing="0" style="padding-top:20px;padding-left:20px;padding-bottom:5px"><tbody>
        <tr> <td>
                <heading>Research Highlight</heading>
          </td> </tr>
      </tbody> </table>

      <table width="100%" align="center" border="0" cellspacing="0" style="padding-top:05px;padding-left:20px;padding-bottom:0px"><tbody>



        
        <tr> <td>
            <div class="grid-container">
            <div>  <!--grid 1-->
               <a href="./2023/REC-MV/">
              <img src="./REC-MV/REC-MV-input.gif" height="100" alt="" /></a>
              <img src="./REC-MV/REC-MV-output.gif" height="100" alt="" /></a>
              <p>CVPR2023: REC-MV: REconstructing 3D Dynamic Cloth from Monocular Videos</p>
            </div>


            <div> <!--grid 3-->
            <a href="./2022/ETHSeg/">
                <img src="./2022/ETHSeg/img_comparison.jpg" height="100" alt="" /></a>
              <p>CVPR2022: ETHSeg: An Amodel Instance Segmentation Network and a Real-world Dataset </p>
            </div>


            <div> <!--grid 2-->
              <a href="https://kv2000.github.io/2022/03/28/reef/">
                <img src="./ReEF/ReEF_1.gif" height="100" alt="" /></a>
                <!-- <img src="./ReEF/ReEF_2.gif" height="100" alt="" /></a> -->
                <p>CVPR2022: ReEF: Towards High-Fidelity Garment mesh Reconstruction from Single Images
                </p>
            </div>

            <div> <!--grid 4-->
              <a href="./2020/OPEC-Net/">
              <img src="./2020/OPEC-Net/media/pipeline.png" height="100" alt="" /></a> 
              <!-- <img src="data/hdr_fire_after.gif" height="100" alt="" /></a> -->
              <p>ECCV2020: Peeking into Occluded Joints: A Novel Framework for Crowd Pose Estimation</p>
            </div>


            <!-- <div> 
              <a href="https://kv2000.github.io/2022/03/28/reef/">
                <img src="./ReEF/ReEF_1.gif" height="100" alt="" /></a>
                <p>CVPR2022: ReEF: Towards High-Fidelity Garment mesh Reconstruction from Single Images
                </p>
            </div>
            
            <div> 
              <img src="./media/tmp.jpg" height="100" alt="" /></a>
              <p>2023<br> Comming Soon </p>
            </div> -->

          </div>
          </td> </tr>
      </tbody> </table>





      <table width="100%" align="center" border="0" cellspacing="0" style="padding-top:20px;padding-left:20px;padding-bottom:5px"><tbody>
        <tr> <td>
                <heading>Publications</heading>  (<strong><sup>#</sup></strong> corresponding author, <strong>*</strong> equal contribution)
                <!--<heading>Publications</heading>  (<strong><sup>&dagger;</sup></strong> corresponding author, <strong>*</strong> equal contribution)-->
          </td> </tr>
      </tbody> </table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <td style="padding:10px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='./media/richdreamer_after.gif' style="height:100%;width:100%; position: absolute;top: -0%; left:25%">
              </div>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D</papertitle>
              <br>
              <strong>Lingteng Qiu*</strong>, 
              <a href="https://guanyingc.github.io/"> Guanying Chen*</a>, 
              <a href="https://scholar.google.com.hk/citations?user=aJPO514AAAAJ&hl=zh-CN&oi=ao">Xiaodong Gu*</a>,
              Qi Zuo,
              <a href="https://mutianxu.github.io/">Mutian Xu</a>, 
              Yushuang Wu,
              <a href=" https://weihao-yuan.com/">Weihao Yuan</a>,
              <a href="https://scholar.google.com/citations?user=GHOQKCwAAAAJ&hl=zh-CN&oi=ao">Zilong Dong</a>,
              <a href="https://research.cs.washington.edu/istc/lfb/">Liefeng Bo</a>,
              <a href="https://mypage.cuhk.edu.cn/academics/hanxiaoguang/">Xiaoguang Han</a>
              <br>
              <em>arXiv preprint</em>, 2023.
              <br>
              <a href="https://arxiv.org/abs/2311.16918">[PDF]</a>
              <a href="https://lingtengqiu.github.io/RichDreamer/">[Project]</a>
              <a href="https://github.com/alibaba/RichDreamer">[Code]</a><a href="https://github.com/alibaba/RichDreamer" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/alibaba/RichDreamer?style=social"></a>
              </div>
            </td>
        </tr>
        

        
        <td style="padding:10px;width:25%;vertical-align:middle">
          <div class="one" >
              <img src='./media/sampro3d.jpg' style="height:40%;width:160%; position: absolute;top: 30%;">
          </div>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          <papertitle>SAMPro3D: Locating SAM Prompts in 3D for Zero-Shot Scene Segmentation</papertitle>
          <br>
          <a href="https://mutianxu.github.io/">Mutian Xu</a>, 
          Xingyilang Yin,
          <strong>Lingteng Qiu</strong>, 
          <a href="https://www.microsoft.com/en-us/research/people/yangliu/">Yang liu</a>,
          <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a>,
          <a href="https://mypage.cuhk.edu.cn/academics/hanxiaoguang/">Xiaoguang Han</a>
          <br>
          <em>arXiv preprint</em>, 2023.
          <br>
          <a href="https://arxiv.org/abs/2311.17707">[PDF]</a>
          <a href="https://mutianxu.github.io/sampro3d/">[Project]</a>
          <a href="https://github.com/GAP-LAB-CUHK-SZ/SAMPro3D">[Code]</a><a href="https://github.com/GAP-LAB-CUHK-SZ/SAMPro3D" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/GAP-LAB-CUHK-SZ/SAMPro3D?style=social"></a>
          </div>
        </td>
    </tr>

      

          <tr></tr>
          <td style="padding:10px;width:25%;vertical-align:middle">
            <div class="one" >
                <img src='./REC-MV/REC-MV.gif' style="height:100%;width:140%; position: absolute;top: -0%">
            </div>
          </td>


          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>REC-MV: REconstructing 3D Dynamic Cloth from Monocular Videos</papertitle>
              <br>
              <strong>Lingteng Qiu<sup>*</sup></strong>, <a href="https://guanyingc.github.io/">Guanying Chen<sup>*</sup></a>, Jiapeng Zhou, <a href="https://mutianxu.github.io/">MutianXu</a>, Junle Wang,
               <a href="https://mypage.cuhk.edu.cn/academics/hanxiaoguang/">Xiaoguang Han<sup>#</sup></a>
              <br>
              <em>2023 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2023, 
              <br>
              <a href="./2023/REC-MV/">[Project]</a>
              <a href="https://arxiv.org/abs/2305.14236">[PDF]</a>
              <a href="images/fenerf.txt">[BibTeX]</a>
              <a href="https://github.com/lingtengqiu/REC-MV">[Code]</a><a href="https://github.com/GAP-LAB-CUHK-SZ/REC-MV" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/GAP-LAB-CUHK-SZ/REC-MV?style=social"></a>
              <br>
              <div class="w3-content" style="text-align: justify">
              <p>
              </p>
              </div>
          </td>
      </tr>


        <tr></tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
                <div class="one" >
                    <img src='./2022/ETHSeg/img_comparison.jpg' style="height:100%;width:140%; position: absolute;top: -0%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>ETHSeg: An Amodel Instance Segmentation Network and a Real-world Dataset for X-Ray Waste Inspection</papertitle>
                <br>
                <strong>Lingteng Qiu</strong>, Zhangyang Xiong, Xuhao Wang, KenKun Liu, <a href="https://guanyingc.github.io/">Guanying Chen</a>, <a href="https://mypage.cuhk.edu.cn/academics/hanxiaoguang/">Xiaoguang Han<sup>#</sup></a>, Shuguang Cui
                <br>
                <em>2022 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2022, 
                <br>
                <a href="./2022/ETHSeg/">[Project]</a>
                <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Qiu_ETHSeg_An_Amodel_Instance_Segmentation_Network_and_a_Real-World_Dataset_CVPR_2022_paper.pdf">[PDF]</a>
                <a href="https://drive.google.com/file/d/1maV_P5vFahWvOi3a7EHdhMpaI9fV8Ed9/view">[Dataset]</a>
				        <a href="images/fenerf.txt">[BibTeX]</a>
                <br>
                <div class="w3-content" style="text-align: justify">
                <p>
                </p>
                </div>
            </td>
        </tr>


        <tr></tr>
        <td style="padding:10px;width:25%;vertical-align:middle">
              <div class="one" >
                <video playsinline autoplay loop preload muted style="width:140%; position: absolute;top: -0%">
                            <source src='./ReEF/ReEF.mp4'>
                </video>
            </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle> Registering Explicit to Implicit:
              Towards High-Fidelity Garment mesh Reconstruction from Single Images </papertitle>
            <br>
            <a herf="https://people.mpi-inf.mpg.de/~hezhu/">Heming Zhu</a>, <strong>Lingteng Qiu</strong>, Yuda Qiu, <a href="https://mypage.cuhk.edu.cn/academics/hanxiaoguang/">Xiaoguang Han<sup>#</sup></a>
            <br>
            <em>2022 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2022, 
            <br>
            <a href="https://kv2000.github.io/2022/03/28/reef/">[Project]</a>
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Registering_Explicit_to_Implicit_Towards_High-Fidelity_Garment_Mesh_Reconstruction_From_CVPR_2022_paper.pdf">[PDF]</a>
            <a href="./ReEF/ReEF.txt">[BibTeX]</a>
            <a href="https://kv2000.github.io/2022/03/28/reef/">[Code]</a>
            <br>
            <div class="w3-content" style="text-align: justify">
            <p> 
            </p>
            </div>
        </td>
    </tr>


    <!-- cvpr2021 -->
    <tr></tr>
    <td style="padding:10px;width:25%;vertical-align:middle">
          <div class="one" >
            <div class="one" >
              <img src='./2021/3DCariShop/paper-teaser.jpg' style="height:100%;width:140%; position: absolute;top: -0%">
            </div>
            </video>
        </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
        <papertitle> 3DCaricShop: A Dataset and A Baseline Method for Single-view 3D Caricature Face Reconstruction </papertitle>
        <br>
        Yuda Qiu, Xiaojie Xu, <strong>Lingteng Qiu</strong>, Yan Pan, Yushuang Wu, Weikai Chen, <a href="https://mypage.cuhk.edu.cn/academics/hanxiaoguang/">Xiaoguang Han<sup>#</sup></a>
        <br>
        <em>2021 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2021, 
        <br>
        <a href="./2021/3DCariShop/">[Project]</a>
        <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Qiu_3DCaricShop_A_Dataset_and_a_Baseline_Method_for_Single-View_3D_CVPR_2021_paper.pdf">[PDF]</a>
        <a href="https://github.com/qiuyuda/3DCaricShop">[Dataset]</a>
        <a href="./2021/3DCariShop/3d_carishop.txt">[BibTeX]</a>
        <a href="https://github.com/qiuyuda/3DCaricShop">[Code]</a>
        <br>
        <div class="w3-content" style="text-align: justify">
        <p> 
        </p>
        </div>
    </td>
    <!-- eccv2022 -->
    <tr></tr>
    <td style="padding:10px;width:25%;vertical-align:middle">
          <div class="one" >
            <div class="one" >
              <img src='./2020/OPEC-Net/media/pipeline.png' style="height:100%;width:140%; position: absolute;top: -0%">
            </div>
            </video>
        </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
        <papertitle> Peeking into Occluded Joints: A Novel Framework for Crowd Pose Estimation </papertitle>
        <br>
        <strong>Lingteng Qiu</strong>, Yanran Li, Guanbin Li ,Xiaojun Wu, Zixiang Xiong,
        <a href="https://mypage.cuhk.edu.cn/academics/hanxiaoguang/">Xiaoguang Han</a><sup>#</sup>, Shuguang Cui
        <br>
        <em>2020 European Conference on Computer Vision</em>, ECCV 2020, 
        <br>
        <a href="./2020/OPEC-Net/">[Project]</a>
        <a href="https://arxiv.org/pdf/2003.10506.pdf">[PDF]</a>
        <a href="./2020/OPEC-Net/opec.txt">[BibTeX]</a>
        <a href="https://github.com/lingtengqiu/OPEC-Net">[Code]</a>
        <a href="https://github.com/lingtengqiu/OPEC-Net" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/lingtengqiu/OPEC-Net?style=social"></a>
        <br>
        <div class="w3-content" style="text-align: justify">
        <p> 
        </p>
        </div>
    </td>



    <!-- project -->

    
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Projects</heading>
      </td>
    </tr>
  </tbody></table>

  <table style="width:100%;border:0px; border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <!-- open-pifuhd -->
    <tr></tr>
    <td style="padding:10px;width:25%;vertical-align:middle">
          <div class="one" >
            <div class="one" >
              <img src='./project/open-pifuhd.png' style="height:60%;width:140%; position: absolute;top: 5%">
            </div>
            </video>
        </div>
    </td>
    <td style="padding:20px ;width:75%;vertical-align:middle">
        <papertitle> Open-PIFuhd</papertitle><br>
        This is an implementation of PIFuhd based on PyTorch, including rendering image normal map, training both coarse-PIFu 
        and fine-PIFu.<br>
        <a href="https://github.com/lingtengqiu/Open-PIFuhd">[Code]</a>
        <a href="https://github.com/lingtengqiu/Open-PIFuhd" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/lingtengqiu/Open-PIFuhd?style=social"></a>
        <br>
        <br>
        <br>
    </td>

    <!-- deeperlab -->
    <tr></tr>
    <td style="padding:10px;width:25%;vertical-align:middle">
          <div class="one" >
            <div class="one" >
              <img src='./project/DeeperLab.png' style="height:60%;width:140%; position: absolute;top: 5%">
            </div>
            </video>
        </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
        <papertitle> DeeperLab</papertitle><br>
        This project aims at providing a fast, modular reference implementation for semantic segmentation models using PyTorch.<br>
        <a href="https://github.com/lingtengqiu/Deeperlab-pytorch">[Code]</a>
        <a href="https://github.com/lingtengqiu/Deeperlab-pytorch" target="_blank"><img class="imgbadge" src="https://img.shields.io/github/stars/lingtengqiu/Deeperlab-pytorch?style=social"></a>
        <br>
        <br>
        <br>
    </td></tr>
  </table>



 <!-- talk -->    
 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
    <heading>Talks</heading>
  </td>
</tr>
</tbody></table>

<table style="width:100%;border:0px; border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<!-- REC-MV -->
<tr></tr>
<td style="padding:10px;width:25%;vertical-align:middle">
      <div class="one" >
        <div class="one" >
          <img src='./media/talk_to_adobe-2023-3-31.png' style="height:60%;width:140%; position: absolute;top: 5%">
        </div>
        </video>
    </div>
</td>

<td style="padding:20px;width:75%;vertical-align:middle">
    <papertitle> REC-MV: REconstructing 3D Dynamic Cloth from Monucular Videos</papertitle><br>
    Invited by Dr. <a href="https://zhouyisjtu.github.io/">Yi Zhou</a><br>
    Digital Humans Seminar, Adobe, 2023<br>
    <a href="https://docs.google.com/presentation/d/1Tz1ltWEWeU3TpqOlaCOqjdAJg_mzp0hC/edit?usp=sharing&ouid=109533105742014760844&rtpof=true&sd=true">[PPT Download]</a>
    <br>
    <br>
    <br>
</td></tr>
</table>






  <table width="100%" align="center" border="0" cellspacing="0" style="padding-top:20px;padding-left:20px;padding-bottom:5px"><tbody>
      <tr> <td>
          <heading>Current Collaborators</heading>
        </td> </tr>
    </tbody> </table>
  <table width="100%" align="center" border="0" cellspacing="0" style="padding-top:00px;padding-left:20px;padding-bottom:5px"><tbody>
      <tr> <td>
          • <a href="https://guanyingc.github.io/">Guanying Chen</a>, Research Assistant Professor at CUHK-SZ</br>
        </td> </tr>
    </tbody>



    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr> <td style="padding:0px"> <br>
         <p>                   The website template was adapted from <a href="https://jonbarron.info/">Jon Barron</a>.</p>
        </td> </tr>
    </tbody> </table>
   

    
  </table>



</body>

</html>
